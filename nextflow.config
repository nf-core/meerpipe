/*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    nf-core/meerpipe Nextflow config file
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    Default config options for all compute environments
----------------------------------------------------------------------------------------
*/

manifest {
    // The metadata of the pipeline
    name = 'MeerPipe'
    homePage = 'https://github.com/OZGrav/meerpipe'
    description = 'Pulsar timing data processing pipeline for MeerTime data.'
    mainScript = 'meerpipe.nf'
    version = '3.0.0'
    author = 'Nick Swainston, Andrew Cameron, Aditya Parthasarathy, Stefan Oslowski, Andrew Jameson, RenÃ©e Spiewak, Daniel Reardon, Matthew Bailes'
    defaultBranch = 'main'
    doi = '10.5281/zenodo.7918680'
}
params.manifest = manifest





// Global default params, used in configs
params {
    params.input = null // Not used, but required for nf-core/configs

    // Observation selection options
    obs_csv  = ""
    utcs     = ""
    utce     = ""
    project  = ""
    pulsar   = ""

    // Processing options
    use_prev_ar = false // If true grab a previous archive file from output directory
    use_edge_subints = false // Use first and last 8 second subints of observation archives
    use_max_nsub = false // Use the maximum number of subints possible for the observation based on the S/N ratio
    tos_sn = 12 // Desired TOA S/N ratio, used to calculate the nsub to use
    nchans = "1" // List of nchans to frequency scrunch the data into
    npols = "1,4" // List of number of polarisations to scrunch the data into
    chop_edge = true // Chop edge frequency channels


    // MeerTime database options
    upload = true  // Upload result to the database
    psrdb_url   = "$PSRDB_URL" // URL for interacting with the database API
    psrdb_token = "$PSRDB_TOKEN" // Token taken from enviroment variable and obtained using get_ingest_token.sh or get_token.sh


    // Directory options (input/output)
    input_dir = "/fred/oz005/timing" // Path to the directories for each pulsar
    outdir    = "/fred/oz005/users/nswainst/meerpipe_testing_outputs" //Path where the data products are stored

    // Options taken from the Configuration file for MeerTime pipeline (meerpipe)
    type = "meertime" //Type of data

    // Ephermiris and template locations. This can be overridden by a user to use none default files
    ephemeris = ""
    template  = ""



    // Boilerplate options
    tracedir                   = "${params.outdir}/pipeline_info"
    publish_dir_mode           = 'copy'
    email                      = null
    email_on_fail              = null
    plaintext_email            = false
    monochrome_logs            = false
    hook_url                   = null
    help                       = false
    version                    = false
    validate_params            = true
    show_hidden_params         = false
    schema_ignore_params       = 'genomes,validationSchemaIgnoreParams,manifest,validation-schema-ignore-params'
    validationShowHiddenParams = false
    validationSchemaIgnoreParams = false


    // Config options
    custom_config_version      = 'master'
    custom_config_base         = "https://raw.githubusercontent.com/nf-core/configs/${params.custom_config_version}"
    config_profile_description = null
    config_profile_contact     = null
    config_profile_url         = null
    config_profile_name        = null


    // Max resource options
    // Defaults only, expecting to be overwritten
    max_memory                 = '250.GB'
    max_cpus                   = 16
    max_time                   = '240.h'

}

// Load base.config by default for all pipelines
includeConfig 'conf/base.config'

// Load nf-core custom profiles from different Institutions
try {
    includeConfig "${params.custom_config_base}/nfcore_custom.config"
} catch (Exception e) {
    System.err.println("WARNING: Could not load nf-core/config profiles: ${params.custom_config_base}/nfcore_custom.config")
}

// Load nf-core/meerpipe custom profiles from different institutions.
// Warning: Uncomment only if a pipeline-specific instititutional config already exists on nf-core/configs!
// try {
//   includeConfig "${params.custom_config_base}/pipeline/meerpipe.config"
// } catch (Exception e) {
//   System.err.println("WARNING: Could not load nf-core/config/meerpipe profiles: ${params.custom_config_base}/pipeline/meerpipe.config")
// }

def mem_calc(dur, task_attempt, ram_slope, ram_max) {
    // Return memory required by the job MB
    // dur is in seconds
    // task_attempt is the attempt number of the task
    // ram slope is is the ram gradiant in GB/s
    // ram_max is maximum ram available for th job in GB
    ram_min       = 2.0 // GB
    ram_max       = ram_max.split("\\.")[0].toFloat() * task_attempt
    ram_intercept = 2.0 // GB

    reqram = task_attempt * Float.valueOf(dur) * ram_slope + ram_intercept

    // Give even more to long observations
    if ( Float.valueOf(dur) > 3600 ) {
        reqram *= 3
    }

    if ( reqram < ram_min ) {
        reqram = ram_min
    }
    else if ( reqram > ram_max ) {
        reqram = ram_max
    }
    return (int) ( reqram )
}

// Software versions
psrdb_version = "3.0.3"
tempo2_version = "0759584_temponest_apar_56e2b58"
meerpipe_version = "0f4aeb9"
psrchive_version = "ac21ad819"
meertime_ephemerides_and_templates_version = "6da4d10"


// CLUSTER SPECFIC DEFAULTS
// ----------------------------------------------------------------------------
def hostname = "hostname".execute().text.trim().replace("-", "")
params.hostname = hostname
if ( hostname.startsWith("tooarrana") || hostname.startsWith("trevor") ) {
    // Set up for OzStars new Ngarrgu Tindebeek cluster

    // Set up containers
    // process.module = 'apptainer'
    // singularity {
    //     enabled = true
    //     runOptions = '--nv -B /nvmetmp'
    //     envWhitelist = 'SINGULARITY_BINDPATH, SINGULARITYENV_LD_LIBRARY_PATH'
    // }
    // params.containerDir = '/pawsey/mwa/singularity'

    // Max resources for the milan partition
    params {
        max_memory                 = '512.GB'
        max_cpus                   = 62
        max_time                   = '7.day'
    }

    // Default directories
    workDir = "/fred/oz005/users/${USER}/work"

    process {
        cache = 'lenient'
        // Resource set up
        withLabel: process_single {
            maxForks = 1
            errorStrategy = 'retry'
            maxRetries = 2
        }
        withLabel: process_low {
            executor = 'slurm'
            errorStrategy = 'retry'
            maxRetries = 2
            queue  = 'milan'
            cpus   = 1
            time   = "600 s"
            memory = "1 GB"
        }
        withLabel: process_high {
            executor = 'slurm'
            errorStrategy = 'retry'
            maxRetries = 2
            queue  = 'milan'
            cpus   = 1
            time   = { "${task.attempt * meta.dur.toFloat() + 300} s" }
            memory = { "${mem_calc(meta.dur, task.attempt, 0.02, '64.GB')} GB" }
        }
        withLabel: scratch {
            scratch = '$JOBFS'
            clusterOptions = { "--tmp=${(task.attempt * meta.dur.toFloat() * 16).toInteger() + 50}MB" }
        }
        // Software dependency set up
        withLabel: psrdb {
            if ( hostname.startsWith("tooarrana") ) {
                beforeScript = "module use /apps/users/pulsar/milan/gcc-11.3.0/modulefiles; module load psrdb/${psrdb_version}"
            }
            else if ( hostname.startsWith("trevor") ) {
                beforeScript = "export SYS_ARCH=openstack; module use /apps/users/pulsar/openstack/gcc-11.3.0/modulefiles; module load psrdb/${psrdb_version}"
            }
        }
        withLabel: psrdb_tempo2 {
            if ( hostname.startsWith("tooarrana") ) {
                beforeScript = "module use /apps/users/pulsar/milan/gcc-11.3.0/modulefiles; module use /apps/users/pulsar/common/modulefiles; module load psrdb/${psrdb_version} tempo2/${tempo2_version}"
            }
            else if ( hostname.startsWith("trevor") ) {
                beforeScript = "export SYS_ARCH=openstack; module use /apps/users/pulsar/openstack/gcc-11.3.0/modulefiles; module use /apps/users/pulsar/common/modulefiles; module load psrdb/${psrdb_version} tempo2/${tempo2_version}"
            }
        }
        withLabel: meerpipe {
            beforeScript = "module use /apps/users/pulsar/milan/gcc-11.3.0/modulefiles; module use /apps/users/pulsar/common/modulefiles; module load meerpipe/${meerpipe_version}"
        }
        withLabel: psrchive {
            beforeScript = "module use /apps/users/pulsar/milan/gcc-11.3.0/modulefiles; module use /apps/users/pulsar/common/modulefiles; module load psrchive/${psrchive_version}"
        }
        withLabel: ephem_template {
            beforeScript = "module use /apps/users/pulsar/milan/gcc-11.3.0/modulefiles; module load meertime_ephemerides_and_templates/${meertime_ephemerides_and_templates_version}"
        }
    }
    executor.submitRateLimit = '100 sec'
    executor.exitReadTimeout = '3 hours'
    executor.$slurm.queueSize = 1000

}
else if ( hostname.startsWith("farnarkle") ) {
    // Set up for Swinburnes's Ozstar cluster
}
else {
    // No recognised hostname so assuming defaults

    // Resource set up
    executor.name = 'local'
    executor.queueSize = 8

    // Set up containers
    docker.enabled = true
}


profiles {
    debug {
        dumpHashes             = true
        process.beforeScript   = 'echo $HOSTNAME'
        cleanup = false
    }
    conda {
        conda.enabled          = true
        docker.enabled         = false
        singularity.enabled    = false
        podman.enabled         = false
        shifter.enabled        = false
        charliecloud.enabled   = false
        apptainer.enabled      = false
    }
    mamba {
        conda.enabled          = true
        conda.useMamba         = true
        docker.enabled         = false
        singularity.enabled    = false
        podman.enabled         = false
        shifter.enabled        = false
        charliecloud.enabled   = false
        apptainer.enabled      = false
    }
    docker {
        docker.enabled         = true
        docker.registry        = 'quay.io'
        docker.userEmulation   = true
        conda.enabled          = false
        singularity.enabled    = false
        podman.enabled         = false
        shifter.enabled        = false
        charliecloud.enabled   = false
        apptainer.enabled      = false
    }
    arm {
        docker.runOptions = '-u $(id -u):$(id -g) --platform=linux/amd64'
    }
    singularity {
        singularity.enabled    = true
        singularity.autoMounts = true
        conda.enabled          = false
        docker.enabled         = false
        podman.enabled         = false
        shifter.enabled        = false
        charliecloud.enabled   = false
        apptainer.enabled      = false
    }
    podman {
        podman.enabled         = true
        podman.registry        = 'quay.io'
        conda.enabled          = false
        docker.enabled         = false
        singularity.enabled    = false
        shifter.enabled        = false
        charliecloud.enabled   = false
        apptainer.enabled      = false
    }
    shifter {
        shifter.enabled        = true
        conda.enabled          = false
        docker.enabled         = false
        singularity.enabled    = false
        podman.enabled         = false
        charliecloud.enabled   = false
        apptainer.enabled      = false
    }
    charliecloud {
        charliecloud.enabled   = true
        conda.enabled          = false
        docker.enabled         = false
        singularity.enabled    = false
        podman.enabled         = false
        shifter.enabled        = false
        apptainer.enabled      = false
    }
    apptainer {
        apptainer.enabled      = true
        conda.enabled          = false
        docker.enabled         = false
        singularity.enabled    = false
        podman.enabled         = false
        shifter.enabled        = false
        charliecloud.enabled   = false
    }
    gitpod {
        executor.name          = 'local'
        executor.cpus          = 16
        executor.memory        = 60.GB
    }
    test      { includeConfig 'conf/test.config'      }
    test_full { includeConfig 'conf/test_full.config' }
    devel {
        params {
            psrdb_url   = "http://localhost:8000/"
            psrdb_token = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VybmFtZSI6Im5pY2siLCJleHAiOjE3MDQ4NzU3MjYsIm9yaWdJYXQiOjE3MDQ4NzU0MjZ9.Wg0jYvHRHVuGv9xbQeFhvZXjfP3EdrWTRIanUaPTd4k"
        }
        process {
            // Software dependency set up
            withLabel: psrdb {
                beforeScript = "module use /apps/users/pulsar/milan/gcc-11.3.0/modulefiles; module use /apps/users/pulsar/common/modulefiles; module load meertime_ephemerides_and_templates/6da4d10; source ~/venv/bin/activate"
            }
            withLabel: psrdb_tempo2 {
                beforeScript = "module use /apps/users/pulsar/milan/gcc-11.3.0/modulefiles; module use /apps/users/pulsar/common/modulefiles; module load tempo2/2023.05.1_temponest_apar_56e2b58; source ~/venv/bin/activate"
            }
            withLabel: meerpipe {
                beforeScript = 'module use /apps/users/pulsar/milan/gcc-11.3.0/modulefiles; module use /apps/users/pulsar/common/modulefiles; module load scintools/1334918 meertime_ephemerides_and_templates/6da4d10; source ~/venv/bin/activate'
            }
            withLabel: psrchive {
                beforeScript = 'module use /apps/users/pulsar/milan/gcc-11.3.0/modulefiles; module use /apps/users/pulsar/common/modulefiles; module load psrchive/e99fec329'
            }
            withLabel: ephem_template {
                beforeScript = 'module use /apps/users/pulsar/milan/gcc-11.3.0/modulefiles; module load meertime_ephemerides_and_templates/6da4d10'
            }
        }
    }
}



// Export these variables to prevent local Python/R libraries from conflicting with those in the container
// The JULIA depot path has been adjusted to a fixed path `/usr/local/share/julia` that needs to be used for packages in the container.
// See https://apeltzer.github.io/post/03-julia-lang-nextflow/ for details on that. Once we have a common agreement on where to keep Julia packages, this is adjustable.

env {
    PYTHONNOUSERSITE = 1
    R_PROFILE_USER   = "/.Rprofile"
    R_ENVIRON_USER   = "/.Renviron"
    JULIA_DEPOT_PATH = "/usr/local/share/julia"
}

// Capture exit codes from upstream processes when piping
process.shell = ['/bin/bash', '-euo', 'pipefail']

def trace_timestamp = new java.util.Date().format( 'yyyy-MM-dd_HH-mm-ss')
timeline {
    enabled = true
    file    = "${params.tracedir}/execution_timeline_${trace_timestamp}.html"
}
report {
    enabled = true
    file    = "${params.tracedir}/execution_report_${trace_timestamp}.html"
}
trace {
    enabled = true
    file    = "${params.tracedir}/execution_trace_${trace_timestamp}.txt"
}
dag {
    enabled = true
    file    = "${params.tracedir}/pipeline_dag_${trace_timestamp}.html"
}

manifest {
    name            = 'nf-core/meerpipe'
    author          = """Nick Swainston"""
    homePage        = 'https://github.com/nf-core/meerpipe'
    description     = """Pulsar timing data processing pipeline for MeerTime data."""
    mainScript      = 'main.nf'
    nextflowVersion = '!>=22.10.1'
    version         = '1.0dev'
    doi             = '10.5281/zenodo.7918680'
}
params.manifest = manifest

// Load modules.config for DSL2 module specific options
includeConfig 'conf/modules.config'

// Function to ensure that resource requirements don't go beyond
// a maximum limit
def check_max(obj, type) {
    if (type == 'memory') {
        try {
            if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
                return params.max_memory as nextflow.util.MemoryUnit
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'time') {
        try {
            if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
                return params.max_time as nextflow.util.Duration
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'cpus') {
        try {
            return Math.min( obj, params.max_cpus as int )
        } catch (all) {
            println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
            return obj
        }
    }
}
